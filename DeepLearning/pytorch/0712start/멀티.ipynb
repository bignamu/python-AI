{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11966abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중선형 회귀 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# data \n",
    "x1_train = torch.FloatTensor(([73], [93], [89], [96], [73]))\n",
    "x2_train = torch.FloatTensor(([80], [88], [91], [98], [66]))\n",
    "x3_train = torch.FloatTensor(([75], [93], [90], [100], [70]))\n",
    "\n",
    "# 정답지 \n",
    "y_train = torch.FloatTensor(([152], [185], [180], [196], [142]))\n",
    "                            \n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aa225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w 와 편향 b를 선언 \n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f956a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/4000 w1 0.848 w2 0.488 w3 0.675 b 0.015 loss 0.337224\n",
      "Epoch  100/4000 w1 0.849 w2 0.487 w3 0.675 b 0.016 loss 0.333976\n",
      "Epoch  200/4000 w1 0.851 w2 0.486 w3 0.674 b 0.016 loss 0.330879\n",
      "Epoch  300/4000 w1 0.852 w2 0.485 w3 0.674 b 0.016 loss 0.327910\n",
      "Epoch  400/4000 w1 0.853 w2 0.484 w3 0.674 b 0.016 loss 0.325068\n",
      "Epoch  500/4000 w1 0.855 w2 0.483 w3 0.673 b 0.016 loss 0.322351\n",
      "Epoch  600/4000 w1 0.856 w2 0.482 w3 0.673 b 0.016 loss 0.319751\n",
      "Epoch  700/4000 w1 0.857 w2 0.481 w3 0.673 b 0.016 loss 0.317259\n",
      "Epoch  800/4000 w1 0.858 w2 0.480 w3 0.672 b 0.016 loss 0.314867\n",
      "Epoch  900/4000 w1 0.860 w2 0.479 w3 0.672 b 0.016 loss 0.312586\n",
      "Epoch 1000/4000 w1 0.861 w2 0.478 w3 0.671 b 0.016 loss 0.310396\n",
      "Epoch 1100/4000 w1 0.862 w2 0.478 w3 0.671 b 0.017 loss 0.308290\n",
      "Epoch 1200/4000 w1 0.863 w2 0.477 w3 0.671 b 0.017 loss 0.306263\n",
      "Epoch 1300/4000 w1 0.864 w2 0.476 w3 0.670 b 0.017 loss 0.304324\n",
      "Epoch 1400/4000 w1 0.865 w2 0.476 w3 0.670 b 0.017 loss 0.302453\n",
      "Epoch 1500/4000 w1 0.866 w2 0.475 w3 0.669 b 0.017 loss 0.300661\n",
      "Epoch 1600/4000 w1 0.867 w2 0.474 w3 0.669 b 0.017 loss 0.298942\n",
      "Epoch 1700/4000 w1 0.868 w2 0.474 w3 0.668 b 0.017 loss 0.297275\n",
      "Epoch 1800/4000 w1 0.869 w2 0.473 w3 0.668 b 0.017 loss 0.295676\n",
      "Epoch 1900/4000 w1 0.870 w2 0.472 w3 0.668 b 0.017 loss 0.294138\n",
      "Epoch 2000/4000 w1 0.871 w2 0.472 w3 0.667 b 0.018 loss 0.292652\n",
      "Epoch 2100/4000 w1 0.872 w2 0.471 w3 0.667 b 0.018 loss 0.291220\n",
      "Epoch 2200/4000 w1 0.873 w2 0.471 w3 0.666 b 0.018 loss 0.289839\n",
      "Epoch 2300/4000 w1 0.874 w2 0.470 w3 0.666 b 0.018 loss 0.288503\n",
      "Epoch 2400/4000 w1 0.875 w2 0.470 w3 0.665 b 0.018 loss 0.287217\n",
      "Epoch 2500/4000 w1 0.876 w2 0.469 w3 0.665 b 0.018 loss 0.285960\n",
      "Epoch 2600/4000 w1 0.877 w2 0.469 w3 0.665 b 0.018 loss 0.284751\n",
      "Epoch 2700/4000 w1 0.878 w2 0.468 w3 0.664 b 0.018 loss 0.283595\n",
      "Epoch 2800/4000 w1 0.879 w2 0.468 w3 0.664 b 0.018 loss 0.282462\n",
      "Epoch 2900/4000 w1 0.880 w2 0.468 w3 0.663 b 0.018 loss 0.281367\n",
      "Epoch 3000/4000 w1 0.880 w2 0.467 w3 0.663 b 0.019 loss 0.280308\n",
      "Epoch 3100/4000 w1 0.881 w2 0.467 w3 0.662 b 0.019 loss 0.279275\n",
      "Epoch 3200/4000 w1 0.882 w2 0.467 w3 0.662 b 0.019 loss 0.278275\n",
      "Epoch 3300/4000 w1 0.883 w2 0.466 w3 0.661 b 0.019 loss 0.277303\n",
      "Epoch 3400/4000 w1 0.884 w2 0.466 w3 0.661 b 0.019 loss 0.276357\n",
      "Epoch 3500/4000 w1 0.884 w2 0.466 w3 0.661 b 0.019 loss 0.275440\n",
      "Epoch 3600/4000 w1 0.885 w2 0.465 w3 0.660 b 0.019 loss 0.274543\n",
      "Epoch 3700/4000 w1 0.886 w2 0.465 w3 0.660 b 0.019 loss 0.273671\n",
      "Epoch 3800/4000 w1 0.887 w2 0.465 w3 0.659 b 0.019 loss 0.272823\n",
      "Epoch 3900/4000 w1 0.887 w2 0.464 w3 0.659 b 0.019 loss 0.271998\n",
      "Epoch 4000/4000 w1 0.888 w2 0.464 w3 0.658 b 0.020 loss 0.271186\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "epoch_num = 4000\n",
    "\n",
    "for epoch in range(epoch_num + 1): \n",
    "    # H(x) 계산 \n",
    "    # 가설을 선언 부분 \n",
    "    # hypothersis = x1_train * w1 + x2_train * w2 + x3_train * w3 _b \n",
    "    hypothersis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    # loss \n",
    "    loss = torch.mean((hypothersis - y_train) ** 2)\n",
    "    \n",
    "    # loss H(x) 계산 \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번 마다 로그 출력 \n",
    "    if epoch % 100 == 0 :\n",
    "        print(\"Epoch {:4d}/{} w1 {:.3f} w2 {:.3f} w3 {:.3f} b {:.3f} loss {:.6f}\"\n",
    "              .format(epoch, epoch_num, w1.item(), w2.item(), w3.item(), b.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43764b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
